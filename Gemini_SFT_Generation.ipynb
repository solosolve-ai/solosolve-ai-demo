{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "27caf9ad208644488a8e7709891eef60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfd6a9f4d4304601b2026a9ac772a731",
              "IPY_MODEL_bd405c55f0014a8fa2f025389b90c540",
              "IPY_MODEL_ef41d4423ec44470b2afb62a5eaa32f0"
            ],
            "layout": "IPY_MODEL_9e7c693388f4496e92e63b2e7361cae8",
            "tabbable": null,
            "tooltip": null
          }
        },
        "bfd6a9f4d4304601b2026a9ac772a731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_3ba2a7e792b147a7860b14cf309b11e3",
            "placeholder": "​",
            "style": "IPY_MODEL_2e902a5953a64983a4e334b2ecad03e3",
            "tabbable": null,
            "tooltip": null,
            "value": "Generating Revised SFT Entries:   6%"
          }
        },
        "bd405c55f0014a8fa2f025389b90c540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b47919db22654143bbbc41872c1cbf31",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49559fb7cea5408d8ed2c2e3f599f66b",
            "tabbable": null,
            "tooltip": null,
            "value": 6
          }
        },
        "ef41d4423ec44470b2afb62a5eaa32f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_a5b75f225d5d44f593ed0bf6ce78ed8a",
            "placeholder": "​",
            "style": "IPY_MODEL_515d2d6a2196422e92ff93462a7df001",
            "tabbable": null,
            "tooltip": null,
            "value": " 6/100 [01:11&lt;17:42, 11.30s/it]"
          }
        },
        "9e7c693388f4496e92e63b2e7361cae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba2a7e792b147a7860b14cf309b11e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e902a5953a64983a4e334b2ecad03e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "b47919db22654143bbbc41872c1cbf31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49559fb7cea5408d8ed2c2e3f599f66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5b75f225d5d44f593ed0bf6ce78ed8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "515d2d6a2196422e92ff93462a7df001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solosolve-ai/solosolve-ai-demo/blob/main/Gemini_SFT_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3e4iz5e-Bk0"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Please ensure you have imported a Gemini API key from AI Studio.\n",
        "You can do this directly in the Secrets tab on the left.\n",
        "\n",
        "After doing so, please run the setup cell below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9FlqlQadyD0",
        "outputId": "7f08ba10-49ac-40b4-c0dd-f2390abb5ecf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-5Pq510-Bk1",
        "outputId": "2928b52b-936f-48a2-830e-a10770674d68"
      },
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO_wzQ9X-Bk2"
      },
      "source": [
        "# Generated Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2petdxH-Bk2",
        "outputId": "58076e27-704a-4151-9cda-c686aeda6232"
      },
      "source": [
        "# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from tqdm.auto import tqdm # <--- ADD THIS LINE\n",
        "\n",
        "\n",
        "def generate():\n",
        "    client = genai.Client(\n",
        "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        "    )\n",
        "\n",
        "    model = \"gemini-2.5-flash-preview-04-17\"\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                types.Part.from_text(text=\"\"\"INSERT_INPUT_HERE\"\"\"),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    generate_content_config = types.GenerateContentConfig(\n",
        "        response_mime_type=\"text/plain\",\n",
        "    )\n",
        "\n",
        "    for chunk in client.models.generate_content_stream(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    ):\n",
        "        print(chunk.text, end=\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you've included the placeholder `INSERT_INPUT_HERE` in your prompt.\n",
            "\n",
            "Please replace `INSERT_INPUT_HERE` with the actual text, question, or task you'd like me to help you with.\n",
            "\n",
            "I'm ready when you provide your input!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Snippet 1: Installs and Setup (Modified for datasets)\n",
        "!pip install -U -q \"google-generativeai\" \"datasets\" \"pandas\" \"ipywidgets\" # ipywidgets for TQDM progress bars in datasets"
      ],
      "metadata": {
        "id": "ifAbNUIuA9ki"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Snippet 1: Setup, Definitions, and Prompt Templates\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "import google.generativeai as genai\n",
        "# from google.generativeai import types # Only if your genai version needs it for GenerationConfig\n",
        "from datasets import load_dataset # If you load data in this snippet\n",
        "import random\n",
        "from datetime import datetime, timedelta # If you use these here\n",
        "\n",
        "# --- Google Drive and API Key Setup ---\n",
        "try:\n",
        "    os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "    if not os.environ[\"GEMINI_API_KEY\"]:\n",
        "        raise ValueError(\"GOOGLE_API_KEY not found in Colab secrets.\")\n",
        "    print(\"GEMINI_API_KEY loaded.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading GEMINI_API_KEY: {e}\")\n",
        "    print(\"Please ensure GOOGLE_API_KEY is set in Colab Secrets (sidebar -> secrets).\")\n",
        "    # For local testing without secrets, you might temporarily set it:\n",
        "    # os.environ[\"GEMINI_API_KEY\"] = \"YOUR_ACTUAL_API_KEY_HERE\"\n",
        "\n",
        "try:\n",
        "    drive.mount(\"/content/drive\", force_remount=True) # force_remount can be helpful\n",
        "    GOOGLE_AI_STUDIO_FOLDER = \"/content/drive/MyDrive/Google AI Studio/AmazonFashionSFT\" # Specific for Fashion\n",
        "    os.makedirs(GOOGLE_AI_STUDIO_FOLDER, exist_ok=True)\n",
        "    os.chdir(GOOGLE_AI_STUDIO_FOLDER)\n",
        "    print(f\"Working directory changed to: {os.getcwd()}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting drive or changing directory: {e}\")\n",
        "    print(f\"Will use current Colab ephemeral storage for outputs: {os.getcwd()}\")\n",
        "    GOOGLE_AI_STUDIO_FOLDER = \".\"\n",
        "\n",
        "# --- Configure Gemini Client ---\n",
        "try:\n",
        "    # Configure once per session is usually enough\n",
        "    if not getattr(genai, '_is_configured_s1_main', False): # Unique flag for this cell\n",
        "        genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\", \"MISSING_KEY\")) # Use .get for safety\n",
        "        genai._is_configured_s1_main = True\n",
        "        print(\"Gemini client configured in Snippet 1.\")\n",
        "    else:\n",
        "        print(\"Gemini client already configured.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini client in Snippet 1: {e}\")\n",
        "\n",
        "# --- SFT Dataset Generation Configuration (Global Constants) ---\n",
        "SAMPLES_TO_GENERATE_FROM_CURATED = 100\n",
        "OUTPUT_SFT_FOLDER = os.path.join(GOOGLE_AI_STUDIO_FOLDER, \"sft_output_amazon_fashion\")\n",
        "os.makedirs(OUTPUT_SFT_FOLDER, exist_ok=True)\n",
        "# The SFT_DATASET_FILE will be specifically named for the revised run in Snippet 3\n",
        "DEFAULT_SFT_FILENAME = \"amazon_fashion_sft_data.jsonl\" # A base name\n",
        "REVISED_SFT_FILENAME = \"amazon_fashion_sft_data_revised.jsonl\"\n",
        "\n",
        "# For Snippet 2 (Data Loading)\n",
        "CURATED_COMPLAINTS_FILE = os.path.join(GOOGLE_AI_STUDIO_FOLDER, \"curated_fashion_complaints.parquet\")\n",
        "MAX_SFT_EXAMPLES = 1000 # Max examples to curate if file doesn't exist\n",
        "SEED = 42\n",
        "RAW_DATASET_NAME = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
        "RAW_REVIEW_CONFIG = \"raw_review_Amazon_Fashion\"\n",
        "RAW_META_CONFIG = \"raw_meta_Amazon_Fashion\"\n",
        "BAD_RATING_THRESHOLD = 3.1\n",
        "MIN_REVIEW_TEXT_LENGTH = 50\n",
        "\n",
        "print(f\"Output SFT files will be saved in: {OUTPUT_SFT_FOLDER}\")\n",
        "print(f\"Curated complaints will be at: {CURATED_COMPLAINTS_FILE}\")\n",
        "\n",
        "# --- Mock Amazon Policy and Categories ---\n",
        "AMAZON_POLICY_DICT = {\n",
        "    \"Return Window\": \"Most items sold on Amazon.com can be returned within 30 days of delivery. Some products have different policies or requirements associated with them.\",\n",
        "    \"Non-Returnable Items\": \"Some items cannot be returned, including: digital music, grocery products, customized products, and items classified as hazardous materials.\",\n",
        "    \"Damaged or Defective\": \"If an item arrives damaged, defective, or is not the item you ordered, please contact Customer Service immediately for a refund or replacement. Photographic evidence may be required.\",\n",
        "    \"Fashion Item Returns\": \"Fashion items (clothing, shoes, jewelry, watches) must be returned in new and unworn condition, with all original packaging, tags, and certificates of authenticity (if applicable). Items showing signs of wear or use, or those that have been altered, resized, or damaged by the customer, may not be eligible for return or may incur a restocking fee.\",\n",
        "    \"Refund Process\": \"Once we receive your return, it will be processed within 5-7 business days. Refunds will be issued to the original payment method.\"\n",
        "}\n",
        "COMPLAINT_CATEGORIES = [\"Sizing Issue\", \"Damaged Item\", \"Not as Described\", \"Shipping Problem\", \"Policy Inquiry\", \"Late Delivery\", \"Wrong Item Received\", \"Quality Issue\", \"Return Process Issue\"]\n",
        "DECISION_TYPES = [\"Full_Refund_No_Return\", \"Full_Refund_With_Return\", \"Partial_Refund_No_Return\", \"Partial_Refund_With_Return\", \"Exchange_Offered\", \"Deny_Request_Policy_Violation\", \"Further_Information_Required\", \"Escalate_To_Human_Agent\", \"Provide_Policy_Information\"]\n",
        "EMOTIONAL_TONES_FOR_FORMAL_ANSWER = [\"Empathetic_Standard\", \"Neutral_Direct\", \"Understanding_Apologetic\", \"Firm_Polite\", \"Helpful_Informative\"]\n",
        "\n",
        "# --- Constructing the JSON Schema String and the Master Prompt Template ---\n",
        "\n",
        "# Step 1: Create the raw JSON schema string with its dynamic parts interpolated.\n",
        "# The literal braces for the JSON structure itself are single here.\n",
        "_complaint_categories_str = ', '.join(COMPLAINT_CATEGORIES)\n",
        "_decision_types_str = ', '.join(DECISION_TYPES)\n",
        "_emotional_tones_str = ', '.join(EMOTIONAL_TONES_FOR_FORMAL_ANSWER)\n",
        "\n",
        "_raw_json_schema_text = f\"\"\"{{\n",
        "  \"is_actionable_complaint\": \"<boolean (true if the complaint describes an issue that Amazon can or should act upon based on policy and context; false if it's a general comment, question not needing action, or clearly outside policy)>\",\n",
        "  \"complaint_category\": \"<choose ONE most relevant category from: {_complaint_categories_str}>\",\n",
        "  \"complaint_summary\": \"<concise summary of the complaint, 1-2 sentences. If product images were analyzed and relevant, integrate visual findings briefly.>\",\n",
        "  \"key_entities_from_complaint\": [\"<list of 3-5 key nouns/phrases from complaint text, e.g., 'dress too small', 'broken zipper', 'late delivery'>\"],\n",
        "  \"image_analysis_summary\": \"<Based on 'Image URLs Provided by Customer' (if any): describe visual evidence relevant to the complaint. If no images were provided or they are irrelevant, state 'No relevant images provided/analyzed'. THIS FIELD IS CRITICAL.>\",\n",
        "  \"information_completeness_assessment\": {{\n",
        "    \"is_complete\": \"<boolean (is all information needed to apply policy and make a decision present in the complaint and provided context? E.g., order ID, specific defect details, desired outcome if not clear)>\",\n",
        "    \"missing_information_prompt\": \"<If is_complete is false, formulate a polite and specific question to the customer to get the missing information, e.g., 'Could you please provide your order ID so I can look into this?'. Use 'NA' if is_complete is true.>\"\n",
        "  }},\n",
        "  \"decision_recommendation\": \"<based on policy, complaint, and history, choose ONE decision type from: {_decision_types_str}>\",\n",
        "  \"suggested_refund_percentage\": \"<integer (e.g., 0, 50, 100), logically derived from the policy, complaint severity, and decision_recommendation. E.g., 100 for Full_Refund, 0 for Deny_Request.>\",\n",
        "  \"return_instructions_if_applicable\": \"<specific instructions if a return is needed (e.g., 'Please use the pre-paid label sent to your email to return the item.'), or 'NA' if no return is applicable or decision is pending information.>\",\n",
        "  \"reasoning_for_decision\": \"<VERY DETAILED. Explain step-by-step how you reached the decision. Explicitly cite relevant 'Amazon Return Policy Snippets' by their title (e.g., 'As per the Return Window policy...'). Connect complaint specifics, user history (if relevant), and image_analysis_summary (if relevant) to the policy and your decision. If policy is ambiguous or information is missing, explain that.>\",\n",
        "  \"formal_answer_to_customer\": \"[TONE: <choose ONE tone from: {_emotional_tones_str}>] Dear Customer, ... <craft a full, polite, professional response text for the customer. This response should reflect the decision, reasoning, required actions (if any), and information completeness assessment (e.g., ask for more info if needed).>\"\n",
        "}}\"\"\"\n",
        "print(\"_raw_json_schema_text prepared.\")\n",
        "\n",
        "# Step 2: Create the ESCAPED version for embedding into REVISED_MASTER_PROMPT_TEMPLATE.\n",
        "# All literal braces from _raw_json_schema_text are doubled.\n",
        "ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT = _raw_json_schema_text.replace('{', '{{').replace('}', '}}')\n",
        "print(\"ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT prepared.\")\n",
        "\n",
        "# Step 3: Define REVISED_MASTER_PROMPT_TEMPLATE using the ESCAPED schema.\n",
        "# The placeholders for .format() in THIS template (e.g., {{user_history_str}}) are single braces\n",
        "# because this f-string itself is not being formatted again; its .format() method will be called later.\n",
        "REVISED_MASTER_PROMPT_TEMPLATE = f\"\"\"\n",
        "You are an AI assistant, an Amazon Resolution Expert, tasked with analyzing Amazon Fashion customer complaints.\n",
        "Your goal is to generate a perfect, gold-standard, structured JSON response to guide customer service actions.\n",
        "Adhere strictly to the provided JSON schema and all guidelines.\n",
        "\n",
        "**CONTEXT FOR ANALYSIS:**\n",
        "\n",
        "1.  **AMAZON RETURN POLICY SNIPPETS (Simulated RAG):**\n",
        "    Policy Title: Return Window\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Return Window', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Non-Returnable Items\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Non-Returnable Items', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Damaged or Defective\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Damaged or Defective', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Fashion Item Returns\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Fashion Item Returns', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Refund Process\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Refund Process', 'Not Available')}\"\n",
        "\n",
        "2.  **DECISION GUIDELINES & REFUND RULES:**\n",
        "    - Prioritize exchanges for sizing issues if the item is eligible.\n",
        "    - Full refunds are typically for defective/damaged items, or items not as described where an exchange is not feasible or desired by policy.\n",
        "    - Deny requests that clearly violate return policy (e.g., worn items, past return window without valid reason).\n",
        "    - If information is missing to make a clear decision, request it.\n",
        "    - Consider user history for context but apply policy consistently.\n",
        "\n",
        "3.  **PRODUCT CONTEXT:**\n",
        "    {{product_title}} <!-- Placeholder for .format() -->\n",
        "    Product ASIN: {{product_asin}}\n",
        "    Product Price: ${{product_price}}\n",
        "    Main Category: {{product_main_category}}\n",
        "    Store: {{product_store}}\n",
        "    Key Features: {{product_features}}\n",
        "    Average Rating: {{product_avg_rating}}/5 ({{product_rating_number}} reviews)\n",
        "    Often Bought Together With (ASINs): {{product_bought_together}}\n",
        "\n",
        "4.  **TEMPORAL CONTEXT:**\n",
        "    Simulated Purchase Date: {{purchase_date_str}}\n",
        "    Current Date (Review Date): {{complaint_timestamp_iso}}\n",
        "    (Assess if the complaint falls within a typical 30-day return window from the simulated purchase date, considering the 'Return Window' policy.)\n",
        "\n",
        "5.  **CUSTOMER'S INTERACTION HISTORY (Simulated MCP):**\n",
        "    {{user_history_str}}\n",
        "\n",
        "6.  **CURRENT COMPLAINT:**\n",
        "    Complaint Rating Given by User: {{complaint_rating_given}}\n",
        "    Complaint Title: \"{{complaint_title_text}}\"\n",
        "    Complaint Body: \"{{complaint_body_text}}\"\n",
        "\n",
        "7.  **IMAGE ANALYSIS TASK:**\n",
        "    Image URLs Provided (Product Images for context): {{image_urls_str}}\n",
        "    **Instruction:** If image URLs are provided AND seem relevant to understanding the product or a potential defect described, YOU MUST analyze these images (conceptually, based on URLs) and reflect your findings in the 'image_analysis_summary' field of your JSON output. If no URLs are provided, they are irrelevant (e.g., stock photos not showing a defect), or you cannot analyze them, state 'No relevant images provided/analyzed' or 'Product images reviewed, no specific defect visible relevant to complaint.'\n",
        "\n",
        "**YOUR TASK:**\n",
        "Based on ALL the information above, generate a SINGLE, VALID JSON object adhering EXACTLY to the schema below.\n",
        "Do NOT include any text outside of this JSON object.\n",
        "\n",
        "**REQUIRED JSON OUTPUT SCHEMA:**\n",
        "{ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT}\n",
        "\n",
        "Response (JSON only):\n",
        "\"\"\"\n",
        "print(\"REVISED_MASTER_PROMPT_TEMPLATE prepared.\")\n",
        "\n",
        "# --- Define generate_sft_entry_with_gemini function ---\n",
        "def generate_sft_entry_with_gemini(model_instance_for_gen, prompt_text_for_gen):\n",
        "    if not model_instance_for_gen:\n",
        "        print(\"Gemini model instance not provided to generation function.\") # Use print for non-loop context\n",
        "        return None\n",
        "    try:\n",
        "        gen_config_class = genai.GenerationConfig\n",
        "        if hasattr(genai, 'types') and hasattr(genai.types, 'GenerationConfig'): # Check for older genai.types\n",
        "            gen_config_class = genai.types.GenerationConfig\n",
        "\n",
        "        generation_config_obj = gen_config_class(\n",
        "            temperature=0.3,\n",
        "            response_mime_type=\"text/plain\"\n",
        "        )\n",
        "        response = model_instance_for_gen.generate_content(\n",
        "            contents=prompt_text_for_gen,\n",
        "            generation_config=generation_config_obj\n",
        "        )\n",
        "        if response and response.candidates and response.candidates[0].content.parts:\n",
        "            return response.candidates[0].content.parts[0].text\n",
        "        else:\n",
        "            feedback_msg = \"N/A\"\n",
        "            if response and hasattr(response, 'prompt_feedback') and response.prompt_feedback:\n",
        "                feedback_msg = str(response.prompt_feedback)\n",
        "            elif response and hasattr(response, 'candidates') and not response.candidates:\n",
        "                 feedback_msg = \"No candidates returned.\"\n",
        "            print(f\"Warning: No content parts in Gemini response. Feedback: {feedback_msg}\")\n",
        "            return None\n",
        "    except Exception as exc:\n",
        "        print(f\"Error during Gemini generate_content: {exc}\")\n",
        "        return None\n",
        "print(\"generate_sft_entry_with_gemini function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12p3LyBcA-4s",
        "outputId": "97b86777-3f7d-468a-ec48-768d6ac532d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEMINI_API_KEY loaded.\n",
            "Mounted at /content/drive\n",
            "Working directory changed to: /content/drive/MyDrive/Google AI Studio/AmazonFashionSFT\n",
            "Gemini client configured in Snippet 1.\n",
            "Output SFT files will be saved in: /content/drive/MyDrive/Google AI Studio/AmazonFashionSFT/sft_output_amazon_fashion\n",
            "Curated complaints will be at: /content/drive/MyDrive/Google AI Studio/AmazonFashionSFT/curated_fashion_complaints.parquet\n",
            "_raw_json_schema_text prepared.\n",
            "ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT prepared.\n",
            "REVISED_MASTER_PROMPT_TEMPLATE prepared.\n",
            "generate_sft_entry_with_gemini function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "4iA4XcVdBP1Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Snippet 2: Data Loading (Reviews and Metadata Separately)\n",
        "\n",
        "# --- Constants for data loading (ensure these are defined before use) ---\n",
        "# These should ideally be passed as arguments or loaded from a config if this were a larger project.\n",
        "# For now, we'll redefine them here, assuming Snippet 1 might not have been run or its globals are not accessible.\n",
        "\n",
        "RAW_DATASET_NAME = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
        "RAW_REVIEW_CONFIG = \"raw_review_Amazon_Fashion\"\n",
        "RAW_META_CONFIG = \"raw_meta_Amazon_Fashion\"\n",
        "BAD_RATING_THRESHOLD = 3.1\n",
        "MIN_REVIEW_TEXT_LENGTH = 50\n",
        "\n",
        "# Constants that were originally in Snippet 1 and are needed here:\n",
        "GOOGLE_AI_STUDIO_FOLDER = \"/content/drive/MyDrive/Google AI Studio/AmazonFashionSFT\" # Or your defined path\n",
        "CURATED_COMPLAINTS_FILE = os.path.join(GOOGLE_AI_STUDIO_FOLDER, \"curated_fashion_complaints.parquet\")\n",
        "MAX_SFT_EXAMPLES = 1000 # As defined in Snippet 1\n",
        "SEED = 42 # As defined in Snippet 1\n",
        "SAMPLES_TO_GENERATE_FROM_CURATED = 100 # As defined in Snippet 1\n",
        "\n",
        "\n",
        "print(\"\\n--- Step 1: Loading Raw Review Data & Curating Complaints ---\")\n",
        "\n",
        "# Load full review dataset\n",
        "try:\n",
        "    print(f\"Loading reviews from '{RAW_DATASET_NAME}', config '{RAW_REVIEW_CONFIG}'...\")\n",
        "    review_dataset_raw = load_dataset(RAW_DATASET_NAME, RAW_REVIEW_CONFIG, split='full', trust_remote_code=True)\n",
        "    print(f\"Successfully loaded {len(review_dataset_raw)} raw reviews.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load review dataset: {e}\")\n",
        "    review_dataset_raw = None\n",
        "\n",
        "df_reviews_full = pd.DataFrame()\n",
        "if review_dataset_raw:\n",
        "    df_reviews_full = review_dataset_raw.to_pandas()\n",
        "    print(f\"Converted raw reviews to Pandas DataFrame with shape: {df_reviews_full.shape}\")\n",
        "\n",
        "# Initialize df_complaints_for_sft to an empty DataFrame\n",
        "df_complaints_for_sft = pd.DataFrame()\n",
        "\n",
        "# Apply your curation logic\n",
        "if not df_reviews_full.empty:\n",
        "    df_complaints_filtered = df_reviews_full[\n",
        "        (df_reviews_full['rating'] < BAD_RATING_THRESHOLD) &\n",
        "        (df_reviews_full['text'].astype(str).str.len() >= MIN_REVIEW_TEXT_LENGTH)\n",
        "    ].copy()\n",
        "    print(f\"Found {len(df_complaints_filtered)} potential complaints after filtering.\")\n",
        "\n",
        "    # Ensure the target directory for CURATED_COMPLAINTS_FILE exists\n",
        "    os.makedirs(os.path.dirname(CURATED_COMPLAINTS_FILE), exist_ok=True)\n",
        "\n",
        "    # Logic for saving/loading curated complaints\n",
        "    if not os.path.exists(CURATED_COMPLAINTS_FILE) or (os.path.exists(CURATED_COMPLAINTS_FILE) and len(df_complaints_filtered) > 0 and os.path.getsize(CURATED_COMPLAINTS_FILE) == 0):\n",
        "        # Create or overwrite if file doesn't exist, or if it exists but is empty and we have new filtered complaints\n",
        "        print(f\"'{CURATED_COMPLAINTS_FILE}' does not exist or is empty, and new complaints are available. Creating/Overwriting...\")\n",
        "        num_to_sample_for_saving = min(len(df_complaints_filtered), MAX_SFT_EXAMPLES * 2)\n",
        "        if num_to_sample_for_saving > 0:\n",
        "            df_complaints_sample_to_save = df_complaints_filtered.sample(num_to_sample_for_saving, random_state=SEED)\n",
        "            df_complaints_sample_to_save.to_parquet(CURATED_COMPLAINTS_FILE)\n",
        "            print(f\"Saved {len(df_complaints_sample_to_save)} curated complaints to {CURATED_COMPLAINTS_FILE}\")\n",
        "            # Sample from the newly saved data for the current SFT run\n",
        "            df_complaints_for_sft = df_complaints_sample_to_save.sample(min(len(df_complaints_sample_to_save), SAMPLES_TO_GENERATE_FROM_CURATED), random_state=SEED)\n",
        "        else:\n",
        "            print(\"No complaints to sample after filtering. Curated file not created/updated.\")\n",
        "    elif os.path.exists(CURATED_COMPLAINTS_FILE):\n",
        "        print(f\"Loading existing curated complaints from {CURATED_COMPLAINTS_FILE}\")\n",
        "        try:\n",
        "            df_complaints_loaded = pd.read_parquet(CURATED_COMPLAINTS_FILE)\n",
        "            if not df_complaints_loaded.empty:\n",
        "                 df_complaints_for_sft = df_complaints_loaded.sample(min(len(df_complaints_loaded), SAMPLES_TO_GENERATE_FROM_CURATED), random_state=SEED)\n",
        "            else:\n",
        "                print(f\"Loaded curated complaints file '{CURATED_COMPLAINTS_FILE}' is empty.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading curated complaints file '{CURATED_COMPLAINTS_FILE}': {e}. Will attempt to regenerate if possible.\")\n",
        "            # Fallback: if loading fails and we have filtered data, try to regenerate\n",
        "            if len(df_complaints_filtered) > 0:\n",
        "                num_to_sample_for_saving = min(len(df_complaints_filtered), MAX_SFT_EXAMPLES * 2)\n",
        "                df_complaints_sample_to_save = df_complaints_filtered.sample(num_to_sample_for_saving, random_state=SEED)\n",
        "                df_complaints_sample_to_save.to_parquet(CURATED_COMPLAINTS_FILE)\n",
        "                print(f\"Re-saved {len(df_complaints_sample_to_save)} curated complaints to {CURATED_COMPLAINTS_FILE}\")\n",
        "                df_complaints_for_sft = df_complaints_sample_to_save.sample(min(len(df_complaints_sample_to_save), SAMPLES_TO_GENERATE_FROM_CURATED), random_state=SEED)\n",
        "\n",
        "\n",
        "    if not df_complaints_for_sft.empty:\n",
        "        print(f\"Using {len(df_complaints_for_sft)} complaint samples for SFT data generation.\")\n",
        "    else:\n",
        "        print(\"No complaint samples available for SFT data generation after curation/loading process.\")\n",
        "\n",
        "else:\n",
        "    print(\"Review DataFrame (df_reviews_full) is empty. Cannot proceed with complaint curation.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Step 2: Loading Raw Metadata ---\")\n",
        "all_meta_df_unique = pd.DataFrame() # Initialize\n",
        "try:\n",
        "    print(f\"Loading metadata from '{RAW_DATASET_NAME}', config '{RAW_META_CONFIG}'...\")\n",
        "    meta_dataset_raw = load_dataset(RAW_DATASET_NAME, RAW_META_CONFIG, split='full', trust_remote_code=True)\n",
        "    all_meta_df = meta_dataset_raw.to_pandas()\n",
        "    print(f\"Successfully loaded {len(all_meta_df)} metadata entries. Shape: {all_meta_df.shape}\")\n",
        "\n",
        "    if 'parent_asin' in all_meta_df.columns:\n",
        "        all_meta_df_unique = all_meta_df.drop_duplicates(subset=['parent_asin'], keep='first')\n",
        "        print(f\"Unique metadata entries by parent_asin: {len(all_meta_df_unique)}\")\n",
        "    else:\n",
        "        print(\"Error: 'parent_asin' column not found in metadata. Cannot deduplicate or merge effectively.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load metadata dataset: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpK-GesWAlrc",
        "outputId": "db108164-61d6-4d9a-a60f-cbe54eff5f88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Step 1: Loading Raw Review Data & Curating Complaints ---\n",
            "Loading reviews from 'McAuley-Lab/Amazon-Reviews-2023', config 'raw_review_Amazon_Fashion'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 2500939 raw reviews.\n",
            "Converted raw reviews to Pandas DataFrame with shape: (2500939, 10)\n",
            "Found 549327 potential complaints after filtering.\n",
            "Loading existing curated complaints from /content/drive/MyDrive/Google AI Studio/AmazonFashionSFT/curated_fashion_complaints.parquet\n",
            "Using 100 complaint samples for SFT data generation.\n",
            "\n",
            "--- Step 2: Loading Raw Metadata ---\n",
            "Loading metadata from 'McAuley-Lab/Amazon-Reviews-2023', config 'raw_meta_Amazon_Fashion'...\n",
            "Successfully loaded 826108 metadata entries. Shape: (826108, 16)\n",
            "Unique metadata entries by parent_asin: 826108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "RxVlEjSPBP1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Snippet 3: Merging Data and SFT Generation (Revised with DeepSeek Feedback)\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import google.generativeai as genai\n",
        "from datetime import datetime, timedelta # For temporal context\n",
        "\n",
        "# --- Ensure Gemini is configured (ideally done in Snippet 1) ---\n",
        "if \"GEMINI_API_KEY\" in os.environ and os.environ[\"GEMINI_API_KEY\"] != \"YOUR_API_KEY_HERE_IF_NOT_IN_SECRETS\":\n",
        "    try:\n",
        "        if not getattr(genai, '_is_configured_s3_revised', False):\n",
        "            genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "            genai._is_configured_s3_revised = True\n",
        "            print(\"Gemini configured in Snippet 3 (Revised).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring Gemini in Snippet 3 (Revised): {e}\")\n",
        "else:\n",
        "    print(\"GEMINI_API_KEY not available for configuration in Snippet 3 (Revised).\")\n",
        "\n",
        "# --- Retrieve necessary variables from global scope (set in Snippet 1 & 2) ---\n",
        "df_complaints_for_sft = globals().get('df_complaints_for_sft', pd.DataFrame())\n",
        "all_meta_df_unique = globals().get('all_meta_df_unique', pd.DataFrame())\n",
        "df_reviews_full = globals().get('df_reviews_full', pd.DataFrame())\n",
        "\n",
        "# --- Constants from Snippet 1 needed for SFT generation loop ---\n",
        "# (These were defined in Snippet 1 and are expected to be in globals())\n",
        "AMAZON_POLICY_DICT = globals().get(\"AMAZON_POLICY_DICT\", {})\n",
        "COMPLAINT_CATEGORIES = globals().get(\"COMPLAINT_CATEGORIES\", [])\n",
        "DECISION_TYPES = globals().get(\"DECISION_TYPES\", [])\n",
        "EMOTIONAL_TONES_FOR_FORMAL_ANSWER = globals().get(\"EMOTIONAL_TONES_FOR_FORMAL_ANSWER\", [])\n",
        "\n",
        "SAMPLES_TO_GENERATE_FROM_CURATED = globals().get(\"SAMPLES_TO_GENERATE_FROM_CURATED\", 0)\n",
        "SFT_DATASET_FILE = globals().get(\"SFT_DATASET_FILE\", \"default_sft_output_revised.jsonl\") # New filename\n",
        "OUTPUT_SFT_FOLDER = globals().get(\"OUTPUT_SFT_FOLDER\", \".\")\n",
        "if SFT_DATASET_FILE == \"default_sft_output_revised.jsonl\": # Update if default was used\n",
        "    SFT_DATASET_FILE = os.path.join(OUTPUT_SFT_FOLDER, \"amazon_fashion_sft_data_revised.jsonl\")\n",
        "\n",
        "\n",
        "# --- Revised EXPECTED_JSON_SCHEMA_FOR_PROMPT and MASTER_PROMPT_TEMPLATE ---\n",
        "# (This should be defined in Snippet 1, retrieving from globals here for robustness)\n",
        "_raw_json_schema_text_from_globals = globals().get(\"_raw_json_schema_text\", \"\")\n",
        "ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT = _raw_json_schema_text_from_globals.replace('{', '{{').replace('}', '}}')\n",
        "\n",
        "# --- New MASTER_PROMPT_TEMPLATE incorporating DeepSeek feedback ---\n",
        "REVISED_MASTER_PROMPT_TEMPLATE = f\"\"\"\n",
        "You are an AI assistant, an Amazon Resolution Expert, tasked with analyzing Amazon Fashion customer complaints.\n",
        "Your goal is to generate a perfect, gold-standard, structured JSON response to guide customer service actions.\n",
        "Adhere strictly to the provided JSON schema and all guidelines.\n",
        "\n",
        "**CONTEXT FOR ANALYSIS:**\n",
        "\n",
        "1.  **AMAZON RETURN POLICY SNIPPETS (Simulated RAG):**\n",
        "    Policy Title: Return Window\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Return Window', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Non-Returnable Items\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Non-Returnable Items', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Damaged or Defective\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Damaged or Defective', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Fashion Item Returns\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Fashion Item Returns', 'Not Available')}\"\n",
        "\n",
        "    Policy Title: Refund Process\n",
        "    Policy Text: \"{AMAZON_POLICY_DICT.get('Refund Process', 'Not Available')}\"\n",
        "\n",
        "2.  **DECISION GUIDELINES & REFUND RULES:**\n",
        "    - Prioritize exchanges for sizing issues if the item is eligible.\n",
        "    - Full refunds are typically for defective/damaged items, or items not as described where an exchange is not feasible or desired by policy.\n",
        "    - Deny requests that clearly violate return policy (e.g., worn items, past return window without valid reason).\n",
        "    - If information is missing to make a clear decision, request it.\n",
        "    - Consider user history for context but apply policy consistently.\n",
        "\n",
        "3.  **PRODUCT CONTEXT:**\n",
        "    Product Title: {{product_title}}\n",
        "    Product ASIN: {{product_asin}}\n",
        "    Product Price: ${{product_price}}\n",
        "    Main Category: {{product_main_category}}\n",
        "    Store: {{product_store}}\n",
        "    Key Features: {{product_features}}\n",
        "    Average Rating: {{product_avg_rating}}/5 ({{product_rating_number}} reviews)\n",
        "    Often Bought Together With (ASINs): {{product_bought_together}}\n",
        "\n",
        "4.  **TEMPORAL CONTEXT:**\n",
        "    Simulated Purchase Date: {{purchase_date_str}}\n",
        "    Current Date (Review Date): {{complaint_timestamp_iso}}\n",
        "    (Assess if the complaint falls within a typical 30-day return window from the simulated purchase date, considering the 'Return Window' policy.)\n",
        "\n",
        "5.  **CUSTOMER'S INTERACTION HISTORY (Simulated MCP):**\n",
        "    {{user_history_str}}\n",
        "\n",
        "6.  **CURRENT COMPLAINT:**\n",
        "    Complaint Rating Given by User: {{complaint_rating_given}}\n",
        "    Complaint Title: \"{{complaint_title_text}}\"\n",
        "    Complaint Body: \"{{complaint_body_text}}\"\n",
        "\n",
        "7.  **IMAGE ANALYSIS TASK:**\n",
        "    Image URLs Provided (Product Images for context): {{image_urls_str}}\n",
        "    **Instruction:** If image URLs are provided AND seem relevant to understanding the product or a potential defect described, YOU MUST analyze these images (conceptually, based on URLs) and reflect your findings in the 'image_analysis_summary' field of your JSON output. If no URLs are provided, they are irrelevant (e.g., stock photos not showing a defect), or you cannot analyze them, state 'No relevant images provided/analyzed' or 'Product images reviewed, no specific defect visible relevant to complaint.'\n",
        "\n",
        "**YOUR TASK:**\n",
        "Based on ALL the information above, generate a SINGLE, VALID JSON object adhering EXACTLY to the schema below.\n",
        "Do NOT include any text outside of this JSON object.\n",
        "\n",
        "**REQUIRED JSON OUTPUT SCHEMA:**\n",
        "{ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT}\n",
        "\n",
        "Response (JSON only):\n",
        "\"\"\"\n",
        "\n",
        "# --- Define generate_sft_entry_with_gemini if not already defined ---\n",
        "if 'generate_sft_entry_with_gemini' not in globals():\n",
        "    print(\"Redefining 'generate_sft_entry_with_gemini' in Snippet 3 (Revised) for safety.\")\n",
        "    def generate_sft_entry_with_gemini(model_instance_for_gen, prompt_text_for_gen):\n",
        "        if not model_instance_for_gen:\n",
        "            tqdm.write(\"Gemini model instance not provided to generation function.\")\n",
        "            return None\n",
        "        try:\n",
        "            gen_config_class = genai.GenerationConfig\n",
        "            if hasattr(genai, 'types') and hasattr(genai.types, 'GenerationConfig'):\n",
        "                gen_config_class = genai.types.GenerationConfig\n",
        "            generation_config_obj = gen_config_class(temperature=0.3, response_mime_type=\"text/plain\")\n",
        "            response = model_instance_for_gen.generate_content(\n",
        "                contents=prompt_text_for_gen,\n",
        "                generation_config=generation_config_obj\n",
        "            )\n",
        "            if response and response.candidates and response.candidates[0].content.parts:\n",
        "                return response.candidates[0].content.parts[0].text\n",
        "            else:\n",
        "                feedback_msg = \"N/A\"\n",
        "                if response and hasattr(response, 'prompt_feedback') and response.prompt_feedback: feedback_msg = str(response.prompt_feedback)\n",
        "                elif response and hasattr(response, 'candidates') and not response.candidates: feedback_msg = \"No candidates returned.\"\n",
        "                tqdm.write(f\"Warning: No content parts in Gemini response. Feedback: {feedback_msg}\")\n",
        "                return None\n",
        "        except Exception as exc:\n",
        "            tqdm.write(f\"Error during Gemini generate_content: {exc}\")\n",
        "            return None\n",
        "\n",
        "print(\"\\n--- Step 3: Merging Curated Complaints with Metadata (Revised) ---\")\n",
        "merged_df_for_sft = pd.DataFrame()\n",
        "review_merge_key = None\n",
        "meta_parent_asin_col_in_merged = 'parent_asin'\n",
        "\n",
        "if not df_complaints_for_sft.empty and not all_meta_df_unique.empty:\n",
        "    if 'parent_asin' in df_complaints_for_sft.columns: review_merge_key = 'parent_asin'\n",
        "    elif 'asin' in df_complaints_for_sft.columns:\n",
        "        print(\"Using 'asin' from reviews as merge key for metadata.\")\n",
        "        review_merge_key = 'asin'\n",
        "    else: print(\"Error: Neither 'parent_asin' nor 'asin' found in curated complaints DataFrame. Cannot merge.\")\n",
        "\n",
        "    if review_merge_key and 'parent_asin' in all_meta_df_unique.columns:\n",
        "        merged_df_for_sft = pd.merge(df_complaints_for_sft, all_meta_df_unique, left_on=review_merge_key, right_on='parent_asin', how='left', suffixes=('_review', '_meta'))\n",
        "        print(f\"Merged DataFrame for SFT shape: {merged_df_for_sft.shape}\")\n",
        "        print(\"Columns in merged_df_for_sft:\", merged_df_for_sft.columns.tolist())\n",
        "        if review_merge_key == 'parent_asin' and 'parent_asin_meta' in merged_df_for_sft.columns: meta_parent_asin_col_in_merged = 'parent_asin_meta'\n",
        "        elif 'parent_asin' in merged_df_for_sft.columns: meta_parent_asin_col_in_merged = 'parent_asin'\n",
        "        else: print(f\"Warning: Could not definitively identify metadata's parent_asin column. Defaulting to '{meta_parent_asin_col_in_merged}'.\")\n",
        "        if meta_parent_asin_col_in_merged not in merged_df_for_sft.columns: print(f\"Error: Identified metadata parent_asin column ('{meta_parent_asin_col_in_merged}') not found!\")\n",
        "        else:\n",
        "            print(f\"Using '{meta_parent_asin_col_in_merged}' as the metadata parent_asin column.\")\n",
        "            num_successfully_merged = merged_df_for_sft[meta_parent_asin_col_in_merged].notna().sum()\n",
        "            print(f\"Number of complaints successfully merged with metadata: {num_successfully_merged}\")\n",
        "            critical_meta_cols = ['title_meta', meta_parent_asin_col_in_merged]\n",
        "            if not all(col in merged_df_for_sft.columns for col in critical_meta_cols): print(f\"Warning: Not all critical_meta_cols ({critical_meta_cols}) found. Skipping dropna.\")\n",
        "            else:\n",
        "                merged_df_for_sft.dropna(subset=critical_meta_cols, inplace=True)\n",
        "                print(f\"Shape after dropping rows with missing critical metadata: {merged_df_for_sft.shape}\")\n",
        "    else: print(\"Cannot merge due to missing key columns or empty input DataFrames.\")\n",
        "else: print(\"Curated complaints (df_complaints_for_sft) or unique metadata (all_meta_df_unique) is empty. Skipping merge.\")\n",
        "\n",
        "def get_user_history_str(user_id, current_complaint_timestamp, full_reviews_df, max_history=2):\n",
        "    # (Function definition as provided previously - ensure df_reviews_full is correctly passed)\n",
        "    if full_reviews_df.empty or 'user_id' not in full_reviews_df.columns: return \"User history lookup unavailable (no full review data).\"\n",
        "    user_reviews = full_reviews_df[full_reviews_df['user_id'] == user_id].copy()\n",
        "    if user_reviews.empty: return \"No prior complaint history found for this user in our records.\"\n",
        "    user_reviews['timestamp_dt'] = pd.to_datetime(user_reviews['timestamp'], unit='ms', errors='coerce')\n",
        "    current_ts_dt = pd.to_datetime(current_complaint_timestamp, unit='ms', errors='coerce')\n",
        "    if pd.isna(current_ts_dt): return \"Current complaint timestamp invalid, cannot reliably fetch history.\"\n",
        "    past_reviews = user_reviews[user_reviews['timestamp_dt'] < current_ts_dt].sort_values(by='timestamp_dt', ascending=False)\n",
        "    if past_reviews.empty: return \"No prior complaint history found for this user (older than current complaint).\"\n",
        "    history_str = f\"User has {len(past_reviews)} prior review(s) on record (showing up to {max_history}):\\n\"\n",
        "    for i, (_, row_hist) in enumerate(past_reviews.head(max_history).iterrows()):\n",
        "        history_str += f\"- Review Title: \\\"{row_hist.get('title', 'N/A')}\\\", Rating: {row_hist.get('rating', 'N/A')}, Product ASIN: {row_hist.get('asin', 'N/A')} (Timestamp: {row_hist.get('timestamp_dt', pd.NaT).strftime('%Y-%m-%d') if pd.notna(row_hist.get('timestamp_dt')) else 'N/A'})\\n\"\n",
        "    return history_str.strip()\n",
        "\n",
        "sft_generated_data = []\n",
        "total_start_time = time.time()\n",
        "successful_generations = 0\n",
        "failed_generations = 0\n",
        "gemini_model_instance = None\n",
        "\n",
        "if \"GEMINI_API_KEY\" in os.environ and os.environ[\"GEMINI_API_KEY\"] != \"YOUR_API_KEY_HERE_IF_NOT_IN_SECRETS\":\n",
        "    try:\n",
        "        MODEL_TO_USE = \"gemini-2.5-flash-preview-04-17\"\n",
        "        gemini_model_instance = genai.GenerativeModel(MODEL_TO_USE)\n",
        "        print(f\"Gemini model instance '{MODEL_TO_USE}' initialized.\")\n",
        "    except Exception as e: print(f\"Error initializing Gemini model: {e}\")\n",
        "else: print(\"GEMINI_API_KEY not configured properly. Gemini calls will be skipped.\")\n",
        "\n",
        "if not REVISED_MASTER_PROMPT_TEMPLATE or not ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT : print(\"ERROR: REVISED_MASTER_PROMPT_TEMPLATE or its schema component is empty. Ensure Snippet 1 has run successfully.\")\n",
        "if SAMPLES_TO_GENERATE_FROM_CURATED == 0: print(\"Warning: SAMPLES_TO_GENERATE_FROM_CURATED is 0.\")\n",
        "\n",
        "if not merged_df_for_sft.empty and gemini_model_instance and REVISED_MASTER_PROMPT_TEMPLATE and ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT and SAMPLES_TO_GENERATE_FROM_CURATED > 0:\n",
        "    num_samples_to_process = min(len(merged_df_for_sft), SAMPLES_TO_GENERATE_FROM_CURATED)\n",
        "    print(f\"\\nAttempting to generate {num_samples_to_process} SFT entries using REVISED prompt...\")\n",
        "    for index, row in tqdm(merged_df_for_sft.head(num_samples_to_process).iterrows(), total=num_samples_to_process, desc=\"Generating Revised SFT Entries\"):\n",
        "        entry_start_time = time.time()\n",
        "        user_id = row.get('user_id', 'UnknownUser')\n",
        "        current_timestamp_val = row.get('timestamp_review', row.get('timestamp'))\n",
        "        user_history_str = get_user_history_str(user_id, current_timestamp_val, df_reviews_full)\n",
        "\n",
        "        # Temporal Context\n",
        "        complaint_datetime = pd.to_datetime(current_timestamp_val, unit='ms', errors='coerce')\n",
        "        complaint_timestamp_iso = complaint_datetime.isoformat() if pd.notna(complaint_datetime) else 'N/A'\n",
        "        # Simulate purchase date as 1 to 30 days before review\n",
        "        simulated_purchase_datetime = complaint_datetime - timedelta(days=random.randint(1, 30)) if pd.notna(complaint_datetime) else pd.NaT\n",
        "        purchase_date_str = simulated_purchase_datetime.strftime('%Y-%m-%d') if pd.notna(simulated_purchase_datetime) else 'Unknown'\n",
        "\n",
        "        # Image URLs\n",
        "        product_image_urls = []\n",
        "        images_data = None\n",
        "        for name in ['images_meta', 'images_review', 'images']:\n",
        "            if name in row.index and pd.notna(row[name]): images_data = row[name]; break\n",
        "        if isinstance(images_data, dict):\n",
        "            large_images_list = images_data.get('large')\n",
        "            if isinstance(large_images_list, list) and large_images_list:\n",
        "                product_image_urls.extend([url for url in large_images_list if isinstance(url, str) and url][:3]) # Limit to 3 images\n",
        "        image_urls_str = \", \".join(product_image_urls) if product_image_urls else \"No product images available\"\n",
        "\n",
        "        # Additional Product Context\n",
        "        features_list = row.get('features', []) # 'features' is a list\n",
        "        product_features_str = \", \".join(features_list) if isinstance(features_list, list) and features_list else \"Not available\"\n",
        "\n",
        "        bought_together_list = row.get('bought_together', []) # 'bought_together' might be a list of ASINs\n",
        "        product_bought_together_str = \", \".join(bought_together_list) if isinstance(bought_together_list, list) and bought_together_list else \"Not available\"\n",
        "\n",
        "        current_review_merge_key = review_merge_key if review_merge_key else 'parent_asin'\n",
        "        current_meta_parent_asin_col = meta_parent_asin_col_in_merged if meta_parent_asin_col_in_merged else 'parent_asin'\n",
        "\n",
        "        filled_prompt = REVISED_MASTER_PROMPT_TEMPLATE.format(\n",
        "            user_history_str=user_history_str,\n",
        "            product_title=row.get('title_meta', row.get('title_review', 'N/A')),\n",
        "            product_asin=row.get(current_meta_parent_asin_col, row.get(current_review_merge_key, 'N/A')),\n",
        "            product_price=row.get('price_meta', row.get('price', 'N/A')),\n",
        "            product_main_category=row.get('main_category_meta', row.get('main_category', 'N/A')),\n",
        "            product_store=row.get('store_meta', row.get('store', 'N/A')),\n",
        "            product_features=product_features_str,\n",
        "            product_avg_rating=row.get('average_rating', '?'),\n",
        "            product_rating_number=row.get('rating_number', 0),\n",
        "            product_bought_together=product_bought_together_str,\n",
        "            purchase_date_str=purchase_date_str,\n",
        "            complaint_timestamp_iso=complaint_timestamp_iso,\n",
        "            complaint_rating_given=row.get('rating_review', row.get('rating', 'N/A')),\n",
        "            complaint_title_text=row.get('title_review', row.get('title', 'N/A')),\n",
        "            complaint_body_text=row.get('text_review', row.get('text', '')),\n",
        "            image_urls_str=image_urls_str\n",
        "        )\n",
        "        generated_text = generate_sft_entry_with_gemini(gemini_model_instance, filled_prompt)\n",
        "        if generated_text:\n",
        "            clean_text = generated_text.strip()\n",
        "            if clean_text.startswith(\"```json\"): clean_text = clean_text[7:-3].strip()\n",
        "            elif clean_text.startswith(\"```\"): clean_text = clean_text[3:-3].strip()\n",
        "            try:\n",
        "                parsed_json_output = json.loads(clean_text)\n",
        "                sft_generated_data.append({\"id\": f\"sft_sample_revised_{row.name}\", \"input_prompt_to_gemini\": filled_prompt, \"gemini_json_output\": parsed_json_output})\n",
        "                successful_generations += 1\n",
        "            except json.JSONDecodeError as json_e:\n",
        "                failed_generations += 1\n",
        "                tqdm.write(f\"Failed to parse JSON for sample (Index: {row.name}): {json_e}\")\n",
        "                sft_generated_data.append({\"id\": f\"sft_sample_revised_{row.name}_PARSE_ERROR\", \"input_prompt_to_gemini\": filled_prompt, \"gemini_raw_output\": clean_text})\n",
        "        else:\n",
        "            failed_generations += 1\n",
        "            tqdm.write(f\"Gemini returned no text for sample (Index: {row.name}).\")\n",
        "        time.sleep(random.uniform(0.5, 1.5))\n",
        "else:\n",
        "    # Print reasons for skipping\n",
        "    if not REVISED_MASTER_PROMPT_TEMPLATE or not ESCAPED_JSON_SCHEMA_FOR_MASTER_PROMPT: print(\"ERROR: REVISED_MASTER_PROMPT_TEMPLATE or its schema component is empty.\")\n",
        "    if SAMPLES_TO_GENERATE_FROM_CURATED == 0: print(\"Warning: SAMPLES_TO_GENERATE_FROM_CURATED is 0.\")\n",
        "    if merged_df_for_sft.empty: print(\"merged_df_for_sft is empty.\")\n",
        "    if not gemini_model_instance: print(\"Gemini model instance is not initialized.\")\n",
        "    print(\"\\nSkipping SFT generation due to missing data, uninitialized model, missing prompt template, or 0 samples to generate.\")\n",
        "\n",
        "total_time_taken = time.time() - total_start_time\n",
        "print(f\"\\n--- SFT Data Generation Summary (Revised Prompt) ---\")\n",
        "print(f\"Total time taken: {total_time_taken:.2f} seconds\")\n",
        "print(f\"Successfully generated entries: {successful_generations}\")\n",
        "print(f\"Failed generations/parses: {failed_generations}\")\n",
        "\n",
        "if sft_generated_data:\n",
        "    print(f\"\\nSaving {len(sft_generated_data)} generated SFT entries to {SFT_DATASET_FILE}...\")\n",
        "    os.makedirs(os.path.dirname(SFT_DATASET_FILE), exist_ok=True)\n",
        "    with open(SFT_DATASET_FILE, 'w') as f:\n",
        "        for entry in sft_generated_data: f.write(json.dumps(entry) + '\\n')\n",
        "    print(\"SFT data saved.\")\n",
        "    if sft_generated_data:\n",
        "        print(\"\\n--- First generated SFT entry (Revised Prompt) ---\")\n",
        "        entry = sft_generated_data[0]\n",
        "        print(\"Input Prompt (truncated):\")\n",
        "        prompt_display = str(entry.get(\"input_prompt_to_gemini\", \"\"))\n",
        "        print(prompt_display[:1000] + \"...\") # Print more of the prompt\n",
        "        print(\"\\nGemini Output (JSON or Raw on error):\")\n",
        "        if \"gemini_json_output\" in entry: print(json.dumps(entry[\"gemini_json_output\"], indent=2))\n",
        "        else: print(entry.get(\"gemini_raw_output\", \"Error: No output recorded\"))\n",
        "        print(\"-\" * 50)\n",
        "else: print(\"\\nNo SFT data was generated to save or print (Revised Prompt).\")\n",
        "print(f\"\\nCheck the folder '{OUTPUT_SFT_FOLDER}' for the output file: {SFT_DATASET_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "27caf9ad208644488a8e7709891eef60",
            "bfd6a9f4d4304601b2026a9ac772a731",
            "bd405c55f0014a8fa2f025389b90c540",
            "ef41d4423ec44470b2afb62a5eaa32f0",
            "9e7c693388f4496e92e63b2e7361cae8",
            "3ba2a7e792b147a7860b14cf309b11e3",
            "2e902a5953a64983a4e334b2ecad03e3",
            "b47919db22654143bbbc41872c1cbf31",
            "49559fb7cea5408d8ed2c2e3f599f66b",
            "a5b75f225d5d44f593ed0bf6ce78ed8a",
            "515d2d6a2196422e92ff93462a7df001"
          ]
        },
        "id": "VgUbCi5HENwH",
        "outputId": "6f79d4a3-45f8-4896-a16d-285a3dc0d047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini configured in Snippet 3 (Revised).\n",
            "\n",
            "--- Step 3: Merging Curated Complaints with Metadata (Revised) ---\n",
            "Merged DataFrame for SFT shape: (100, 25)\n",
            "Columns in merged_df_for_sft: ['rating', 'title_review', 'text', 'images_review', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase', 'main_category', 'title_meta', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images_meta', 'videos', 'store', 'categories', 'details', 'bought_together', 'subtitle', 'author']\n",
            "Using 'parent_asin' as the metadata parent_asin column.\n",
            "Number of complaints successfully merged with metadata: 100\n",
            "Shape after dropping rows with missing critical metadata: (100, 25)\n",
            "Gemini model instance 'gemini-2.5-flash-preview-04-17' initialized.\n",
            "\n",
            "Attempting to generate 100 SFT entries using REVISED prompt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Revised SFT Entries:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27caf9ad208644488a8e7709891eef60"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ]
    }
  ]
}